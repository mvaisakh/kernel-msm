/* Copyright (c) 2012-2017, The Linux Foundation. All rights reserved.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 and
 * only version 2 as published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 */

#define pr_fmt(fmt) "%s:%s " fmt, KBUILD_MODNAME, __func__

#include <linux/kernel.h>
#include <linux/init.h>
#include <linux/module.h>
#include <linux/kthread.h>
#include <linux/mutex.h>
#include <linux/msm_tsens.h>
#include <linux/workqueue.h>
#include <linux/completion.h>
#include <linux/cpu.h>
#include <linux/cpufreq.h>
#include <linux/msm_tsens.h>
#include <linux/msm_thermal.h>
#include <linux/platform_device.h>
#include <linux/of.h>
#include <linux/err.h>
#include <linux/slab.h>
#include <linux/of.h>
#include <linux/sysfs.h>
#include <linux/types.h>
#include <linux/thermal.h>
#include <linux/regulator/rpm-smd-regulator.h>
#include <linux/regulator/consumer.h>
#include <linux/regulator/driver.h>
#include <linux/msm_thermal_ioctl.h>
#include <soc/qcom/rpm-smd.h>
#include <soc/qcom/scm.h>
#include <linux/debugfs.h>
#include <linux/pm_opp.h>
#include <linux/sched/rt.h>
#include <linux/notifier.h>
#include <linux/reboot.h>
#include <soc/qcom/msm-core.h>
#include <linux/cpumask.h>
#include <linux/suspend.h>
#include <linux/uaccess.h>
#include <linux/uio_driver.h>
#include <linux/io.h>

#include <asm/cacheflush.h>

#define CREATE_TRACE_POINTS
#define TRACE_MSM_THERMAL
#include <trace/trace_thermal.h>

#define MSM_LIMITS_DCVSH		0x10
#define MSM_LIMITS_NODE_DCVS		0x44435653
#define MSM_LIMITS_SUB_FN_GENERAL	0x47454E00
#define MSM_LIMITS_SUB_FN_CRNT		0x43524E54
#define MSM_LIMITS_SUB_FN_REL		0x52454C00
#define MSM_LIMITS_DOMAIN_MAX		0x444D4158
#define MSM_LIMITS_DOMAIN_MIN		0x444D494E
#define MSM_LIMITS_CLUSTER_0		0x6370302D
#define MSM_LIMITS_CLUSTER_1		0x6370312D
#define MSM_LIMITS_ALGO_MODE_ENABLE	0x454E424C

#define MAX_CURRENT_UA 100000
#define MAX_RAILS 5
#define TSENS_NAME_FORMAT "tsens_tz_sensor%d"
#define THERM_SECURE_BITE_CMD 8
#define SENSOR_SCALING_FACTOR 1
#define MSM_THERMAL_NAME "msm_thermal"
#define MSM_TSENS_PRINT  "log_tsens_temperature"
#define CPU_BUF_SIZE 64
#define CPU_DEVICE "cpu%d"
#define MAX_DEBUGFS_CONFIG_LEN   32
#define MSM_THERMAL_CONFIG        "config"
#define MSM_CONFIG_DATA           "data"
#define DEBUGFS_DISABLE_ALL_MIT   "disable"
#define DEBUGFS_CONFIG_UPDATE     "update"
#define MSM_THERMAL_THRESH        "thresh_degc"
#define MSM_THERMAL_THRESH_CLR    "thresh_clr_degc"
#define MSM_THERMAL_THRESH_UPDATE "update"
#define DEVM_NAME_MAX 30
#define HOTPLUG_RETRY_INTERVAL_MS 100
#define UIO_VERSION "1.0"

#define CXIP_LM_BASE_ADDRESS      0x1FE5000
#define CXIP_LM_ADDRESS_SIZE      0x68
#define CXIP_LM_VOTE_STATUS       0x40
#define CXIP_LM_BYPASS            0x44
#define CXIP_LM_VOTE_CLEAR        0x48
#define CXIP_LM_VOTE_SET          0x4c
#define CXIP_LM_FEATURE_EN        0x50
#define CXIP_LM_DISABLE_VAL       0x0
#define CXIP_LM_BYPASS_VAL        0xFF00
#define CXIP_LM_THERM_VOTE_VAL    0x80
#define CXIP_LM_THERM_SENS_ID     8
#define CXIP_LM_THERM_SENS_HIGH   90
#define CXIP_LM_THERM_SENS_LOW    75

#define VALIDATE_AND_SET_MASK(_node, _key, _mask, _cpu) \
	do { \
		if (of_property_read_bool(_node, _key)) \
			_mask |= BIT(_cpu); \
	} while (0)

#define THERM_CREATE_DEBUGFS_DIR(_node, _name, _parent, _ret) \
	do { \
		_node = debugfs_create_dir(_name, _parent); \
		if (IS_ERR(_node)) { \
			_ret = PTR_ERR(_node); \
			pr_err("Error creating debugfs dir:%s. err:%d\n", \
					_name, _ret); \
		} \
	} while (0)

#define UPDATE_THRESHOLD_SET(_val, _trip) do {		\
	if (_trip == THERMAL_TRIP_CONFIGURABLE_HI)	\
		_val |= 1;				\
	else if (_trip == THERMAL_TRIP_CONFIGURABLE_LOW)\
		_val |= 2;				\
} while (0)

#define UPDATE_CPU_CONFIG_THRESHOLD(_mask, _id, _high, _low) \
	do { \
		int cpu; \
		for_each_possible_cpu(cpu) { \
			if (!(_mask & BIT(cpus[cpu].cpu))) \
				continue; \
			cpus[cpu].threshold[_id].temp = _high \
				* tsens_scaling_factor; \
			cpus[cpu].threshold[_id + 1].temp = _low \
				* tsens_scaling_factor; \
			set_and_activate_threshold( \
				cpus[cpu].sensor_id, \
				&cpus[cpu].threshold[_id]); \
			set_and_activate_threshold( \
				cpus[cpu].sensor_id, \
				&cpus[cpu].threshold[_id + 1]); \
		} \
	} while (0)

static struct msm_thermal_data msm_thermal_info;
static struct delayed_work check_temp_work, retry_hotplug_work;
static bool core_control_enabled;
static uint32_t cpus_offlined;
static cpumask_var_t cpus_previously_online;
static DEFINE_MUTEX(core_control_mutex);
static struct kobject *cc_kobj;
static struct kobject *mx_kobj;
static struct task_struct *hotplug_task;
static struct task_struct *freq_mitigation_task;
static struct task_struct *thermal_monitor_task;
static struct completion hotplug_notify_complete;
static struct completion freq_mitigation_complete;
static struct completion thermal_monitor_complete;

static int enabled;
static int polling_enabled;
static int rails_cnt;
static int sensor_cnt;
static int psm_rails_cnt;
static int ocr_rail_cnt;
static int limit_idx;
static int limit_idx_low;
static int limit_idx_high;
static int max_tsens_num;
static struct cpufreq_frequency_table *table;
static uint32_t usefreq;
static int freq_table_get;
static bool vdd_rstr_enabled;
static bool vdd_rstr_nodes_called;
static bool vdd_rstr_probed;
static bool sensor_info_nodes_called;
static bool sensor_info_probed;
static bool config_info_nodes_called;
static bool config_info_probed;
static char *config_info;
static bool psm_enabled;
static bool psm_nodes_called;
static bool psm_probed;
static bool freq_mitigation_enabled;
static bool boot_freq_mitig_enabled;
static bool ocr_enabled;
static bool ocr_nodes_called;
static bool ocr_probed;
static bool ocr_reg_init_defer;
static bool hotplug_enabled;
static bool msm_thermal_probed;
static bool gfx_crit_phase_ctrl_enabled;
static bool gfx_warm_phase_ctrl_enabled;
static bool cx_phase_ctrl_enabled;
static bool vdd_mx_enabled;
static bool therm_reset_enabled;
static bool cxip_lm_enabled;
static bool online_core;
static bool cluster_info_probed;
static bool cluster_info_nodes_called;
static bool in_suspend, retry_in_progress;
static bool lmh_dcvs_available;
static bool lmh_dcvs_is_supported;
static int *tsens_id_map;
static int *zone_id_tsens_map;
static DEFINE_MUTEX(vdd_rstr_mutex);
static DEFINE_MUTEX(psm_mutex);
static DEFINE_MUTEX(cx_mutex);
static DEFINE_MUTEX(gfx_mutex);
static DEFINE_MUTEX(ocr_mutex);
static DEFINE_MUTEX(vdd_mx_mutex);
static DEFINE_MUTEX(threshold_mutex);
static uint32_t curr_gfx_band;
static uint32_t curr_cx_band;
static struct kobj_attribute cx_mode_attr;
static struct kobj_attribute gfx_mode_attr;
static struct kobj_attribute mx_enabled_attr;
static struct attribute_group cx_attr_gp;
static struct attribute_group gfx_attr_gp;
static struct attribute_group mx_attr_group;
static struct regulator *vdd_mx, *vdd_cx;
static int *tsens_temp_at_panic;
static bool tsens_temp_print;
static uint32_t bucket;
static cpumask_t throttling_mask;
static int tsens_scaling_factor = SENSOR_SCALING_FACTOR;
static void *cxip_lm_reg_base;

static LIST_HEAD(devices_list);
static LIST_HEAD(thresholds_list);
static int mitigation = 1;

enum thermal_threshold {
	HOTPLUG_THRESHOLD_HIGH,
	HOTPLUG_THRESHOLD_LOW,
	FREQ_THRESHOLD_HIGH,
	FREQ_THRESHOLD_LOW,
	THRESHOLD_MAX_NR,
};

struct cluster_info {
	int cluster_id;
	uint32_t entity_count;
	struct cluster_info *child_entity_ptr;
	struct cluster_info *parent_ptr;
	struct cpufreq_frequency_table *freq_table;
	int freq_idx;
	int freq_idx_low;
	int freq_idx_high;
	struct cpumask cluster_cores;
	uint32_t limited_max_freq;
	uint32_t limited_min_freq;
};

struct cpu_info {
	uint32_t cpu;
	const char *sensor_type;
	enum sensor_id_type id_type;
	uint32_t sensor_id;
	bool offline;
	bool user_offline;
	bool hotplug_thresh_clear;
	struct sensor_threshold threshold[THRESHOLD_MAX_NR];
	bool max_freq;
	uint32_t user_max_freq;
	uint32_t shutdown_max_freq;
	uint32_t suspend_max_freq;
	uint32_t vdd_max_freq;
	uint32_t user_min_freq;
	uint32_t limited_max_freq;
	uint32_t limited_min_freq;
	bool freq_thresh_clear;
	struct cluster_info *parent_ptr;
};

struct rail {
	const char			*name;
	uint32_t			freq_req;
	uint32_t			min_level;
	uint32_t			num_levels;
	int32_t				curr_level;
	uint32_t			levels[3];
	struct kobj_attribute		value_attr;
	struct kobj_attribute		level_attr;
	struct regulator		*reg;
	struct attribute_group		attr_gp;
	uint32_t			max_frequency_limit;
	struct device_clnt_data		*device_handle[NR_CPUS];
	union device_request		request[NR_CPUS];
};

struct msm_sensor_info {
	const char *name;
	const char *alias;
	const char *type;
	uint32_t scaling_factor;
};

struct psm_rail {
	const char *name;
	uint8_t init;
	uint8_t mode;
	struct kobj_attribute mode_attr;
	struct rpm_regulator *reg;
	struct regulator *phase_reg;
	struct attribute_group attr_gp;
};

struct devmgr_devices {
	struct device_manager_data *hotplug_dev;
	struct device_manager_data *cpufreq_dev[NR_CPUS];
};

enum msm_thresh_list {
	MSM_THERM_RESET,
	MSM_VDD_RESTRICTION,
	MSM_CX_PHASE_CTRL_HOT,
	MSM_GFX_PHASE_CTRL_WARM,
	MSM_GFX_PHASE_CTRL_HOT,
	MSM_OCR,
	MSM_VDD_MX_RESTRICTION,
	MSM_THERM_CXIP_LM,
	MSM_LIST_MAX_NR,
};

enum msm_thermal_phase_ctrl {
	MSM_CX_PHASE_CTRL,
	MSM_GFX_PHASE_CTRL,
	MSM_PHASE_CTRL_NR,
};

enum msm_temp_band {
	MSM_COLD_CRITICAL = 1,
	MSM_COLD,
	MSM_COOL,
	MSM_NORMAL,
	MSM_WARM,
	MSM_HOT,
	MSM_HOT_CRITICAL,
	MSM_TEMP_MAX_NR,
};

enum cpu_mit_type {
	CPU_FREQ_MITIGATION    = 0x1,
	CPU_HOTPLUG_MITIGATION = 0x2,
};

enum cpu_config {
	HOTPLUG_CONFIG,
	CPUFREQ_CONFIG,
	MAX_CPU_CONFIG
};

enum freq_limits {
	FREQ_LIMIT_MIN = 0x1,
	FREQ_LIMIT_MAX = 0x2,
	FREQ_LIMIT_ALL = 0x3,
};

struct msm_thermal_debugfs_thresh_config {
	char config_name[MAX_DEBUGFS_CONFIG_LEN];
	long thresh;
	long thresh_clr;
	bool update;
	void (*disable_config)(void);
	struct dentry *dbg_config;
	struct dentry *dbg_thresh;
	struct dentry *dbg_thresh_clr;
	struct dentry *dbg_thresh_update;
};

struct msm_thermal_debugfs_entry {
	struct dentry *parent;
	struct dentry *tsens_print;
	struct dentry *config;
	struct dentry *config_data;
};

static struct psm_rail *psm_rails;
static struct psm_rail *ocr_rails;
static struct rail *rails;
static struct msm_sensor_info *sensors;
static struct cpu_info cpus[NR_CPUS];
static struct threshold_info *thresh;
static bool mx_restr_applied;
static struct cluster_info *core_ptr;
static struct msm_thermal_debugfs_entry *msm_therm_debugfs;
static struct devmgr_devices *devices;
static struct msm_thermal_debugfs_thresh_config *mit_config;

struct vdd_rstr_enable {
	struct kobj_attribute ko_attr;
	uint32_t enabled;
};

/* For SMPS only*/
enum PMIC_SW_MODE {
	PMIC_AUTO_MODE  = RPM_REGULATOR_MODE_AUTO,
	PMIC_IPEAK_MODE = RPM_REGULATOR_MODE_IPEAK,
	PMIC_PWM_MODE   = RPM_REGULATOR_MODE_HPM,
};

enum ocr_request {
	OPTIMUM_CURRENT_MIN,
	OPTIMUM_CURRENT_MAX,
	OPTIMUM_CURRENT_NR,
};

static int thermal_config_debugfs_read(struct seq_file *m, void *data);
static ssize_t thermal_config_debugfs_write(struct file *file,
					const char __user *buffer,
					size_t count, loff_t *ppos);

#define VDD_RES_RO_ATTRIB(_rail, ko_attr, j, _name) \
	ko_attr.attr.name = __stringify(_name); \
	ko_attr.attr.mode = 0444; \
	ko_attr.show = vdd_rstr_reg_##_name##_show; \
	ko_attr.store = NULL; \
	sysfs_attr_init(&ko_attr.attr); \
	_rail.attr_gp.attrs[j] = &ko_attr.attr;

#define VDD_RES_RW_ATTRIB(_rail, ko_attr, j, _name) \
	ko_attr.attr.name = __stringify(_name); \
	ko_attr.attr.mode = 0644; \
	ko_attr.show = vdd_rstr_reg_##_name##_show; \
	ko_attr.store = vdd_rstr_reg_##_name##_store; \
	sysfs_attr_init(&ko_attr.attr); \
	_rail.attr_gp.attrs[j] = &ko_attr.attr;

#define VDD_RSTR_ENABLE_FROM_ATTRIBS(attr) \
	(container_of(attr, struct vdd_rstr_enable, ko_attr));

#define VDD_RSTR_REG_VALUE_FROM_ATTRIBS(attr) \
	(container_of(attr, struct rail, value_attr));

#define VDD_RSTR_REG_LEVEL_FROM_ATTRIBS(attr) \
	(container_of(attr, struct rail, level_attr));

#define OCR_RW_ATTRIB(_rail, ko_attr, j, _name) \
	ko_attr.attr.name = __stringify(_name); \
	ko_attr.attr.mode = 0644; \
	ko_attr.show = ocr_reg_##_name##_show; \
	ko_attr.store = ocr_reg_##_name##_store; \
	sysfs_attr_init(&ko_attr.attr); \
	_rail.attr_gp.attrs[j] = &ko_attr.attr;

#define PSM_RW_ATTRIB(_rail, ko_attr, j, _name) \
	ko_attr.attr.name = __stringify(_name); \
	ko_attr.attr.mode = 0644; \
	ko_attr.show = psm_reg_##_name##_show; \
	ko_attr.store = psm_reg_##_name##_store; \
	sysfs_attr_init(&ko_attr.attr); \
	_rail.attr_gp.attrs[j] = &ko_attr.attr;

#define PSM_REG_MODE_FROM_ATTRIBS(attr) \
	(container_of(attr, struct psm_rail, mode_attr));

#define PHASE_RW_ATTR(_phase, _name, _attr, j, _attr_gr) \
	_attr.attr.name = __stringify(_name); \
	_attr.attr.mode = 0644; \
	_attr.show = _phase##_phase_show; \
	_attr.store = _phase##_phase_store; \
	sysfs_attr_init(&_attr.attr); \
	_attr_gr.attrs[j] = &_attr.attr;

#define MX_RW_ATTR(ko_attr, _name, _attr_gp) \
	ko_attr.attr.name = __stringify(_name); \
	ko_attr.attr.mode = 0644; \
	ko_attr.show = show_mx_##_name; \
	ko_attr.store = store_mx_##_name; \
	sysfs_attr_init(&ko_attr.attr); \
	_attr_gp.attrs[0] = &ko_attr.attr;

#define THERM_MITIGATION_DISABLE(_flag, _id) \
	do { \
		if (!_flag) \
			return; \
		if (_id >= 0) \
			sensor_mgr_disable_threshold( \
				&thresh[_id]); \
		_flag = 0; \
	} while (0)

#define APPLY_VDD_RESTRICTION(vdd, level, name, ret)                   \
	do {                                                              \
		ret = regulator_set_voltage(vdd, level, INT_MAX);         \
		if (ret) {                                                \
			pr_err("Failed to vote %s to level %d, err %d\n", \
			 #name, level, ret);                              \
		} else {                                                  \
			ret = regulator_enable(vdd);                      \
			if (ret)                                          \
				pr_err("Failed to enable %s, err %d\n",   \
					#name, ret);                      \
			else                                              \
				pr_debug("Vote %s with level %d\n",       \
					#name, level);                    \
		}                                                         \
	} while (0)

#define REMOVE_VDD_RESTRICTION(vdd, name, ret)                             \
	do {                                                                  \
		ret = regulator_disable(vdd);                                 \
		if (ret) {                                                    \
			pr_err("Failed to disable %s, error %d\n",            \
				#name, ret);                                  \
		} else {                                                      \
			ret = regulator_set_voltage(vdd, 0, INT_MAX);      \
			if (ret)                                              \
				pr_err("Failed to remove %s vote, error %d\n",\
					#name, ret);                          \
			else                                                  \
				pr_debug("Remove voting to %s\n", #name);     \
		}                                                             \
	} while (0)

#define CXIP_LM_CLIENTS_STATUS()                                        \
	readl_relaxed(cxip_lm_reg_base + CXIP_LM_VOTE_STATUS)

static void uio_init(struct platform_device *pdev)
{
	int ret = 0;
	struct uio_info *uio_reg_info = NULL;
	struct resource *clnt_res = NULL;
	u32 mem_size = 0;
	phys_addr_t mem_pyhsical = 0;

	clnt_res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
	if (!clnt_res) {
		pr_debug("resource not found\n");
		goto exit;
	}
	mem_size = resource_size(clnt_res);
	if (mem_size == 0) {
		pr_err("resource memory size is zero\n");
		goto exit;
	}

	uio_reg_info = devm_kzalloc(&pdev->dev, sizeof(struct uio_info),
			GFP_KERNEL);
	if (!uio_reg_info)
		goto exit;

	mem_pyhsical = clnt_res->start;

	/* Setup device */
	uio_reg_info->name = clnt_res->name;
	uio_reg_info->version = UIO_VERSION;
	uio_reg_info->mem[0].addr = mem_pyhsical;
	uio_reg_info->mem[0].size = mem_size;
	uio_reg_info->mem[0].memtype = UIO_MEM_PHYS;

	ret = uio_register_device(&pdev->dev, uio_reg_info);
	if (ret) {
		devm_kfree(&pdev->dev, uio_reg_info);
		pr_err("uio register failed ret=%d\n", ret);
		goto exit;
	}
	dev_set_drvdata(&pdev->dev, uio_reg_info);

exit:
	return;
}

static void get_cluster_mask(uint32_t cpu, cpumask_t *mask)
{
	int i;

	cpumask_set_cpu(cpu, mask);
	if (core_ptr) {
		for (i = 0; i < core_ptr->entity_count; i++) {
			struct cluster_info *cluster_ptr =
				&core_ptr->child_entity_ptr[i];
			if (*cluster_ptr->cluster_cores.bits & BIT(cpu)) {
				cpumask_copy(mask,
					&cluster_ptr->cluster_cores);
				break;
			}
		}
	}
}

static uint32_t get_core_max_freq(uint32_t cpu)
{
	int i;
	uint32_t max_freq = 0;

	if (core_ptr) {
		for (i = 0; i < core_ptr->entity_count; i++) {
			struct cluster_info *cluster_ptr =
				&core_ptr->child_entity_ptr[i];
			if (*cluster_ptr->cluster_cores.bits & BIT(cpu)) {
				if (cluster_ptr->freq_table)
					max_freq =
					cluster_ptr->freq_table
					[cluster_ptr->freq_idx_high].frequency;
				break;
			}
		}
	} else {
		if (table)
			max_freq = table[limit_idx_high].frequency;
	}

	return max_freq;
}

static void cpus_previously_online_update(void)
{
	get_online_cpus();
	cpumask_or(cpus_previously_online, cpus_previously_online,
		   cpu_online_mask);
	put_online_cpus();
	pr_debug("%*pb\n", cpumask_pr_args(cpus_previously_online));
}

static uint32_t get_core_min_freq(uint32_t cpu)
{
	int i;
	uint32_t min_freq = UINT_MAX;

	if (core_ptr) {
		for (i = 0; i < core_ptr->entity_count; i++) {
			struct cluster_info *cluster_ptr =
				&core_ptr->child_entity_ptr[i];
			if (*cluster_ptr->cluster_cores.bits & BIT(cpu)) {
				if (cluster_ptr->freq_table)
					min_freq =
					cluster_ptr->freq_table[0].frequency;
				break;
			}
		}
	} else {
		if (table)
			min_freq = table[0].frequency;
	}

	return min_freq;
}

static void msm_thermal_update_freq(bool is_shutdown, bool mitigate)
{
	uint32_t cpu;
	bool update = false;

	for_each_possible_cpu(cpu) {
		if (msm_thermal_info.freq_mitig_control_mask
			& BIT(cpu)) {
			uint32_t *freq = (is_shutdown)
				? &cpus[cpu].shutdown_max_freq
				: &cpus[cpu].suspend_max_freq;
			uint32_t mitigation_freq = (mitigate) ?
				get_core_min_freq(cpu) : UINT_MAX;

			if (*freq == mitigation_freq)
				continue;
			*freq = mitigation_freq;
			update = true;
			pr_debug("%s mitigate CPU%u to %u\n",
				(is_shutdown) ? "Shutdown" : "Suspend", cpu,
				mitigation_freq);
		}
	}

	if (!update)
		goto notify_exit;

	if (freq_mitigation_task)
		complete(&freq_mitigation_complete);
	else
		pr_err("Freq mitigation task is not initialized\n");
notify_exit:
	return;
}

static int msm_thermal_power_down_callback(
		struct notifier_block *nfb, unsigned long action, void *data)
{

	switch (action) {
	case SYS_RESTART:
	case SYS_POWER_OFF:
	case SYS_HALT:
		msm_thermal_update_freq(true, true);
		break;

	default:
		return NOTIFY_DONE;
	}

	return NOTIFY_OK;
}

static int msm_thermal_suspend_callback(
		struct notifier_block *nfb, unsigned long action, void *data)
{
	switch (action) {
	case PM_HIBERNATION_PREPARE:
	case PM_SUSPEND_PREPARE:
		msm_thermal_update_freq(false, true);
		in_suspend = true;
		retry_in_progress = false;
		cancel_delayed_work_sync(&retry_hotplug_work);
		break;

	case PM_POST_HIBERNATION:
	case PM_POST_SUSPEND:
		msm_thermal_update_freq(false, false);
		in_suspend = false;
		if (hotplug_task)
			complete(&hotplug_notify_complete);
		else
			pr_debug("Hotplug task not initialized\n");
		break;

	default:
		return NOTIFY_DONE;
	}

	return NOTIFY_OK;
}

static struct notifier_block msm_thermal_reboot_notifier = {
	.notifier_call = msm_thermal_power_down_callback,
};

static struct device_manager_data *find_device_by_name(const char *device_name)
{
	struct device_manager_data *dev_mgr = NULL;

	list_for_each_entry(dev_mgr, &devices_list, dev_ptr) {
		if (strcmp(dev_mgr->device_name, device_name) == 0)
			return dev_mgr;
	}

	return NULL;
}

static int validate_client(struct device_clnt_data *clnt)
{
	int ret = 0;
	struct device_manager_data *dev_mgr = NULL;
	struct device_clnt_data *client_ptr = NULL;

	if (!clnt || !clnt->dev_mgr) {
		pr_err("Invalid client\n");
		ret = -EINVAL;
		goto validate_exit;
	}

	list_for_each_entry(dev_mgr, &devices_list, dev_ptr) {
		if (dev_mgr == clnt->dev_mgr)
			break;
	}
	if (dev_mgr != clnt->dev_mgr) {
		pr_err("Invalid device manager\n");
		ret = -EINVAL;
		goto validate_exit;
	}

	mutex_lock(&dev_mgr->clnt_lock);
	list_for_each_entry(client_ptr, &dev_mgr->client_list, clnt_ptr) {
		if (clnt == client_ptr)
			break;
	}
	if (clnt != client_ptr) {
		pr_err("Invalid client\n");
		ret = -EINVAL;
		goto validate_unlock;
	}
validate_unlock:
	mutex_unlock(&dev_mgr->clnt_lock);

validate_exit:
	return ret;
}

static int devmgr_client_cpufreq_update(struct device_manager_data *dev_mgr)
{
	int ret = 0;
	struct device_clnt_data *clnt = NULL;
	uint32_t max_freq = UINT_MAX;
	uint32_t min_freq = 0;

	mutex_lock(&dev_mgr->clnt_lock);
	list_for_each_entry(clnt, &dev_mgr->client_list, clnt_ptr) {
		if (!clnt->req_active)
			continue;
		max_freq = min(max_freq, clnt->request.freq.max_freq);
		min_freq = max(min_freq, clnt->request.freq.min_freq);
	}
	if (dev_mgr->active_req.freq.max_freq == max_freq &&
		dev_mgr->active_req.freq.min_freq == min_freq) {
		goto update_exit;
	}
	dev_mgr->active_req.freq.max_freq = max_freq;
	dev_mgr->active_req.freq.min_freq = min_freq;

	if (freq_mitigation_task) {
		complete(&freq_mitigation_complete);
	} else {
		pr_err("Frequency mitigation task is not initialized\n");
		ret = -ESRCH;
	}

update_exit:
	mutex_unlock(&dev_mgr->clnt_lock);
	return ret;
}

static int devmgr_client_hotplug_update(struct device_manager_data *dev_mgr)
{
	int ret = 0;
	struct device_clnt_data *clnt = NULL;
	cpumask_t offline_mask = CPU_MASK_NONE;

	mutex_lock(&dev_mgr->clnt_lock);
	list_for_each_entry(clnt, &dev_mgr->client_list, clnt_ptr) {
		if (!clnt->req_active)
			continue;
		cpumask_or(&offline_mask, &offline_mask,
				&clnt->request.offline_mask);
	}
	if (cpumask_equal(&dev_mgr->active_req.offline_mask, &offline_mask))
		goto update_exit;

	cpumask_copy(&dev_mgr->active_req.offline_mask, &offline_mask);

	if (hotplug_task) {
		complete(&hotplug_notify_complete);
	} else {
		pr_err("Hotplug task is not initialized\n");
		ret = -ESRCH;
	}

update_exit:
	mutex_unlock(&dev_mgr->clnt_lock);
	return ret;
}

static int devmgr_hotplug_client_request_validate_and_update(
				struct device_clnt_data *clnt,
				union device_request *req,
				enum device_req_type type)
{
	if (type != HOTPLUG_MITIGATION_REQ)
		return -EINVAL;

	cpumask_copy(&clnt->request.offline_mask, &req->offline_mask);

	if (!cpumask_empty(&req->offline_mask))
		clnt->req_active = true;
	else
		clnt->req_active = false;

	return 0;
}

static int devmgr_cpufreq_client_request_validate_and_update(
						struct device_clnt_data *clnt,
						union device_request *req,
						enum device_req_type type)
{
	if (type != CPUFREQ_MITIGATION_REQ)
		return -EINVAL;

	if (req->freq.max_freq < req->freq.min_freq) {
		pr_err("Invalid Max and Min freq req. max:%u min:%u\n",
			req->freq.max_freq, req->freq.min_freq);
		return -EINVAL;
	}

	clnt->request.freq.max_freq = req->freq.max_freq;
	clnt->request.freq.min_freq = req->freq.min_freq;

	if ((req->freq.max_freq == CPUFREQ_MAX_NO_MITIGATION) &&
		(req->freq.min_freq == CPUFREQ_MIN_NO_MITIGATION))
		clnt->req_active = false;
	else
		clnt->req_active = true;

	return 0;
}

int devmgr_client_request_mitigation(struct device_clnt_data *clnt,
					enum device_req_type type,
					union device_request *req)
{
	int ret = 0;
	struct device_manager_data *dev_mgr = NULL;

	if (!mitigation) {
		pr_err("Thermal Mitigations disabled.\n");
		goto req_exit;
	}

	if (!clnt || !req) {
		pr_err("Invalid inputs for mitigation.\n");
		ret = -EINVAL;
		goto req_exit;
	}

	ret = validate_client(clnt);
	if (ret) {
		pr_err("Invalid mitigation client. ret:%d\n", ret);
		goto req_exit;
	}

	if (!clnt->dev_mgr->request_validate) {
		pr_err("Invalid dev mgr request update\n");
		ret = -EINVAL;
		goto req_exit;
	}

	dev_mgr = clnt->dev_mgr;
	mutex_lock(&dev_mgr->clnt_lock);
	ret = dev_mgr->request_validate(clnt, req, type);
	if (ret) {
		pr_err("Invalid client request\n");
		goto req_unlock;
	}

req_unlock:
	mutex_unlock(&dev_mgr->clnt_lock);
	if (!ret && dev_mgr->update)
		dev_mgr->update(dev_mgr);

req_exit:
	return ret;
}

struct device_clnt_data *devmgr_register_mitigation_client(struct device *dev,
				const char *device_name,
				void (*callback)(struct device_clnt_data *,
				union device_request *, void *))
{
	struct device_clnt_data *client = NULL;
	struct device_manager_data *dev_mgr = NULL;

	if (!dev || !device_name) {
		pr_err("Invalid input\n");
		return ERR_PTR(-EINVAL);
	}

	dev_mgr = find_device_by_name(device_name);
	if (!dev_mgr) {
		pr_err("Invalid device %s\n", device_name);
		return ERR_PTR(-EINVAL);
	}

	client = devm_kzalloc(dev,
		sizeof(struct device_clnt_data), GFP_KERNEL);
	if (!client) {
		pr_err("Memory alloc failed\n");
		return ERR_PTR(-ENOMEM);
	}

	mutex_lock(&dev_mgr->clnt_lock);
	client->dev_mgr = dev_mgr;
	client->callback = callback;
	list_add_tail(&client->clnt_ptr, &dev_mgr->client_list);
	mutex_unlock(&dev_mgr->clnt_lock);

	return client;
}

void devmgr_unregister_mitigation_client(struct device *dev,
					struct device_clnt_data *clnt)
{
	int ret = 0;
	struct device_manager_data *dev_mgr = NULL;

	if (!clnt) {
		pr_err("Invalid input\n");
		return;
	}

	ret = validate_client(clnt);
	if (ret)
		return;

	dev_mgr = clnt->dev_mgr;
	mutex_lock(&dev_mgr->clnt_lock);
	list_del(&clnt->clnt_ptr);
	mutex_unlock(&dev_mgr->clnt_lock);
	devm_kfree(dev, clnt);
	if (dev_mgr->update)
		dev_mgr->update(dev_mgr);
}

static int  msm_thermal_cpufreq_callback(struct notifier_block *nfb,
		unsigned long event, void *data)
{
	struct cpufreq_policy *policy = data;
	uint32_t max_freq_req, min_freq_req;

	switch (event) {
	case CPUFREQ_ADJUST:
		max_freq_req = (lmh_dcvs_is_supported) ? UINT_MAX :
			cpus[policy->cpu].parent_ptr->limited_max_freq;
		min_freq_req = cpus[policy->cpu].parent_ptr->limited_min_freq;
		pr_debug("mitigating CPU%d to freq max: %u min: %u\n",
			policy->cpu, max_freq_req, min_freq_req);

		cpufreq_verify_within_limits(policy, min_freq_req,
			max_freq_req);

		if (max_freq_req < min_freq_req)
			pr_err("Invalid frequency request Max:%u Min:%u\n",
				max_freq_req, min_freq_req);
		break;
	}
	return NOTIFY_OK;
}

static struct notifier_block msm_thermal_cpufreq_notifier = {
	.notifier_call = msm_thermal_cpufreq_callback,
};

static int msm_lmh_dcvs_write(uint32_t node_id, uint32_t fn, uint32_t setting,
				uint32_t val)
{
	int ret;
	struct scm_desc desc_arg;
	uint32_t *payload = NULL;

	payload = kzalloc(sizeof(uint32_t) * 5, GFP_KERNEL);
	if (!payload)
		return -ENOMEM;

	payload[0] = fn;
	payload[1] = 0; /* unused sub-algorithm */
	payload[2] = setting;
	payload[3] = 1; /* number of values */
	payload[4] = val;

	desc_arg.args[0] = SCM_BUFFER_PHYS(payload);
	desc_arg.args[1] = sizeof(uint32_t) * 5;
	desc_arg.args[2] = MSM_LIMITS_NODE_DCVS;
	desc_arg.args[3] = node_id;
	desc_arg.args[4] = 0; /* version */
	desc_arg.arginfo = SCM_ARGS(5, SCM_RO, SCM_VAL, SCM_VAL,
					SCM_VAL, SCM_VAL);

	dmac_flush_range(payload, (void *)payload + 5 * (sizeof(uint32_t)));
	ret = scm_call2(SCM_SIP_FNID(SCM_SVC_LMH, MSM_LIMITS_DCVSH), &desc_arg);

	kfree(payload);
	return ret;
}

static int msm_lmh_dcvs_update(int cpu)
{
	uint32_t id = cpus[cpu].parent_ptr->cluster_id;
	uint32_t max_freq = cpus[cpu].limited_max_freq;
	uint32_t min_freq = cpus[cpu].limited_min_freq;
	uint32_t affinity;
	int ret;

	switch (id) {
	case 0:
		affinity = MSM_LIMITS_CLUSTER_0;
		break;
	case 1:
		affinity = MSM_LIMITS_CLUSTER_1;
		break;
	default:
		pr_err("%s: unknown affinity %d\n", __func__, id);
		return -EINVAL;
	};

	ret = msm_lmh_dcvs_write(affinity, MSM_LIMITS_SUB_FN_GENERAL,
					MSM_LIMITS_DOMAIN_MAX, max_freq);
	if (ret)
		return ret;

	ret = msm_lmh_dcvs_write(affinity, MSM_LIMITS_SUB_FN_GENERAL,
					MSM_LIMITS_DOMAIN_MIN, min_freq);
	if (ret)
		return ret;
	/*
	 * Notify LMH dcvs driver about the new software limit. This will
	 * trigger LMH DCVS driver polling for the mitigated frequency.
	 */
	msm_lmh_dcvsh_sw_notify(cpu);

	return ret;
}

static void update_cpu_freq(int cpu, enum freq_limits changed)
{
	int ret = 0;
	cpumask_t mask;

	/*
	 * If the limits overshoot each other, choose the min requirement
	 * over the max freq requirement.
	 */
	if (cpus[cpu].limited_min_freq > cpus[cpu].limited_max_freq)
		cpus[cpu].limited_max_freq = cpus[cpu].limited_min_freq;

	get_cluster_mask(cpu, &mask);
	if (cpu_online(cpu)) {
		if ((cpumask_intersects(&mask, &throttling_mask))
			&& (cpus[cpu].limited_max_freq
				>= get_core_max_freq(cpu))) {
			cpumask_xor(&throttling_mask, &mask, &throttling_mask);
			set_cpu_throttled(&mask, false);
		} else if (!cpumask_intersects(&mask, &throttling_mask)) {
			cpumask_or(&throttling_mask, &mask, &throttling_mask);
			set_cpu_throttled(&mask, true);
		}
		trace_thermal_pre_frequency_mit(cpu,
			cpus[cpu].limited_max_freq,
			cpus[cpu].limited_min_freq);

		/*
		 * If LMH DCVS is available, we update the hardware directly
		 * for faster response. However, the LMH DCVS does not aggregate
		 * min freq correctly - cpufreq could be voting for a min
		 * freq lesser than what we desire and that would be honored.
		 * Update cpufreq, so the min freq remains consistent in the hw.
		 */
		if (lmh_dcvs_available) {
			msm_lmh_dcvs_update(cpu);
			if (changed & FREQ_LIMIT_MIN)
				cpufreq_update_policy(cpu);
		} else {
			cpufreq_update_policy(cpu);
		}

		trace_thermal_post_frequency_mit(cpu,
			cpufreq_quick_get_max(cpu),
			cpus[cpu].limited_min_freq);
		if (ret)
			pr_err("Unable to update policy for cpu:%d. err:%d\n",
				cpu, ret);
	} else if (lmh_dcvs_available) {
		trace_thermal_pre_frequency_mit(cpu,
			cpus[cpu].limited_max_freq,
			cpus[cpu].limited_min_freq);
		msm_lmh_dcvs_update(cpu);
		trace_thermal_post_frequency_mit(cpu,
			cpufreq_quick_get_max(cpu),
			cpus[cpu].limited_min_freq);
	}
}

static ssize_t cluster_info_show(
	struct kobject *kobj, struct kobj_attribute *attr, char *buf)
{
	uint32_t i = 0;
	ssize_t tot_size = 0, size = 0;

	for (; i < core_ptr->entity_count; i++) {
		struct cluster_info *cluster_ptr =
				&core_ptr->child_entity_ptr[i];

		size = snprintf(&buf[tot_size], PAGE_SIZE - tot_size,
			"%d:%lu:1 ", cluster_ptr->cluster_id,
			*cluster_ptr->cluster_cores.bits);
		if ((tot_size + size) >= PAGE_SIZE) {
			pr_err("Not enough buffer size");
			break;
		}
		tot_size += size;
	}

	return tot_size;
}

static int thermal_config_debugfs_open(struct inode *inode,
					struct file *file)
{
	return single_open(file, thermal_config_debugfs_read,
				inode->i_private);
}

static const struct file_operations thermal_debugfs_config_ops = {
	.open = thermal_config_debugfs_open,
	.read = seq_read,
	.write = thermal_config_debugfs_write,
	.llseek = seq_lseek,
	.release = single_release,
};

static int create_config_debugfs(
		struct msm_thermal_debugfs_thresh_config *config_ptr,
		struct dentry *parent)
{
	int ret = 0;

	if (!strlen(config_ptr->config_name))
		return -ENODEV;

	THERM_CREATE_DEBUGFS_DIR(config_ptr->dbg_config,
		config_ptr->config_name, parent, ret);
	if (ret)
		goto create_exit;

	config_ptr->dbg_thresh = debugfs_create_u64(MSM_THERMAL_THRESH,
		0600, config_ptr->dbg_config, (u64 *)&config_ptr->thresh);
	if (IS_ERR(config_ptr->dbg_thresh)) {
		ret = PTR_ERR(config_ptr->dbg_thresh);
		pr_err("Error creating thresh debugfs:[%s]. error:%d\n",
			config_ptr->config_name, ret);
		goto create_exit;
	}

	config_ptr->dbg_thresh_clr = debugfs_create_u64(MSM_THERMAL_THRESH_CLR,
		0600, config_ptr->dbg_config, (u64 *)&config_ptr->thresh_clr);
	if (IS_ERR(config_ptr->dbg_thresh)) {
		ret = PTR_ERR(config_ptr->dbg_thresh);
		pr_err("Error creating thresh_clr debugfs:[%s]. error:%d\n",
			config_ptr->config_name, ret);
		goto create_exit;
	}

	config_ptr->dbg_thresh_update = debugfs_create_bool(
		MSM_THERMAL_THRESH_UPDATE, 0600, config_ptr->dbg_config,
		&config_ptr->update);
	if (IS_ERR(config_ptr->dbg_thresh_update)) {
		ret = PTR_ERR(config_ptr->dbg_thresh_update);
		pr_err("Error creating enable debugfs:[%s]. error:%d\n",
			config_ptr->config_name, ret);
		goto create_exit;
	}

create_exit:
	if (ret)
		debugfs_remove_recursive(parent);

	return ret;
}

static int create_thermal_debugfs(void)
{
	int ret = 0, idx = 0;

	if (msm_therm_debugfs)
		return ret;

	msm_therm_debugfs = devm_kzalloc(&msm_thermal_info.pdev->dev,
			sizeof(struct msm_thermal_debugfs_entry), GFP_KERNEL);
	if (!msm_therm_debugfs) {
		ret = -ENOMEM;
		pr_err("Memory alloc failed. err:%d\n", ret);
		return ret;
	}

	THERM_CREATE_DEBUGFS_DIR(msm_therm_debugfs->parent, MSM_THERMAL_NAME,
		NULL, ret);
	if (ret)
		goto create_exit;

	msm_therm_debugfs->tsens_print = debugfs_create_bool(MSM_TSENS_PRINT,
			0600, msm_therm_debugfs->parent, &tsens_temp_print);
	if (IS_ERR(msm_therm_debugfs->tsens_print)) {
		ret = PTR_ERR(msm_therm_debugfs->tsens_print);
		pr_err("Error creating debugfs:[%s]. err:%d\n",
			MSM_TSENS_PRINT, ret);
		goto create_exit;
	}

	THERM_CREATE_DEBUGFS_DIR(msm_therm_debugfs->config, MSM_THERMAL_CONFIG,
		msm_therm_debugfs->parent, ret);
	if (ret)
		goto create_exit;

	msm_therm_debugfs->config_data = debugfs_create_file(MSM_CONFIG_DATA,
			0600, msm_therm_debugfs->config, NULL,
			&thermal_debugfs_config_ops);
	if (!msm_therm_debugfs->config_data) {
		pr_err("Error creating debugfs:[%s]\n",
			MSM_CONFIG_DATA);
		goto create_exit;
	}
	for (idx = 0; idx < MSM_LIST_MAX_NR + MAX_CPU_CONFIG; idx++)
		create_config_debugfs(&mit_config[idx],
			msm_therm_debugfs->config);

create_exit:
	if (ret) {
		debugfs_remove_recursive(msm_therm_debugfs->parent);
		devm_kfree(&msm_thermal_info.pdev->dev, msm_therm_debugfs);
	}
	return ret;
}

static struct kobj_attribute cluster_info_attr = __ATTR_RO(cluster_info);
static int create_cpu_topology_sysfs(void)
{
	int ret = 0;
	struct kobject *module_kobj = NULL;

	if (!cluster_info_probed) {
		cluster_info_nodes_called = true;
		return ret;
	}
	if (!core_ptr)
		return ret;

	module_kobj = kset_find_obj(module_kset, KBUILD_MODNAME);
	if (!module_kobj) {
		pr_err("cannot find kobject\n");
		return -ENODEV;
	}

	sysfs_attr_init(&cluster_info_attr.attr);
	ret = sysfs_create_file(module_kobj, &cluster_info_attr.attr);
	if (ret) {
		pr_err("cannot create cluster info attr group. err:%d\n", ret);
		return ret;
	}

	return ret;
}

static int get_device_tree_cluster_info(struct device *dev, int *cluster_id,
			cpumask_t *cluster_cpus)
{
	int idx = 0, ret = 0, max_entry = 0, core_cnt = 0, c_idx = 0, cpu = 0;
	uint32_t val = 0;
	char *key = "qcom,synchronous-cluster-map";
	struct device_node *core_phandle = NULL;

	if (!of_get_property(dev->of_node, key, &max_entry)
		|| max_entry <= 0) {
		pr_debug("Property %s not defined.\n", key);
		return -ENODEV;
	}
	max_entry /= sizeof(__be32);

	for (idx = 0; idx < max_entry; idx++, c_idx++) {
		/* Read Cluster ID */
		ret = of_property_read_u32_index(dev->of_node, key, idx++,
			&val);
		if (ret) {
			pr_err("Error reading index%d. err:%d\n", idx - 1,
				ret);
			return -EINVAL;
		}
		/* Read number of cores inside a cluster */
		cluster_id[c_idx] = val;
		cpumask_clear(&cluster_cpus[c_idx]);
		ret = of_property_read_u32_index(dev->of_node, key, idx,
			&val);
		if (ret || val < 1) {
			pr_err("Invalid core count[%d] for Cluster%d. err:%d\n"
					, val, cluster_id[c_idx - 1], ret);
			return -EINVAL;
		}
		core_cnt = val + idx;
		/* map the cores to logical CPUs and get sibiling mask */
		for (; core_cnt != idx; core_cnt--) {
			core_phandle = of_parse_phandle(dev->of_node, key,
						core_cnt);
			if (!core_phandle) {
				pr_debug("Invalid phandle. core%d cluster%d\n",
					core_cnt, cluster_id[c_idx - 1]);
				continue;
			}

			for_each_possible_cpu(cpu) {
				if (of_get_cpu_node(cpu, NULL)
					== core_phandle)
					break;
			}
			if (cpu >= num_possible_cpus()) {
				pr_debug("Skipping core%d in cluster%d\n",
					core_cnt, cluster_id[c_idx - 1]);
				continue;
			}
			cpumask_set_cpu(cpu, &cluster_cpus[c_idx]);
		}
		idx += val;
	}

	return c_idx;
}

static int get_kernel_cluster_info(int *cluster_id, cpumask_t *cluster_cpus)
{
	uint32_t _cpu, cluster_index, cluster_cnt;

	for (_cpu = 0, cluster_cnt = 0; _cpu < num_possible_cpus(); _cpu++) {
		if (topology_physical_package_id(_cpu) < 0) {
			pr_err("CPU%d topology not initialized.\n", _cpu);
			return -ENODEV;
		}
		/* Do not use the sibling cpumask from topology module.
		** kernel topology module updates the sibling cpumask
		** only when the cores are brought online for the first time.
		** KTM figures out the sibling cpumask using the
		** cluster and core ID mapping.
		*/
		for (cluster_index = 0; cluster_index < num_possible_cpus();
			cluster_index++) {
			if (cluster_id[cluster_index] == -1) {
				cluster_id[cluster_index] =
					topology_physical_package_id(_cpu);
				cpumask_clear(&cluster_cpus[cluster_index]);
				cpumask_set_cpu(_cpu,
					&cluster_cpus[cluster_index]);
				cluster_cnt++;
				break;
			}
			if (cluster_id[cluster_index] ==
				topology_physical_package_id(_cpu)) {
				cpumask_set_cpu(_cpu,
					&cluster_cpus[cluster_index]);
				break;
			}
		}
	}

	return cluster_cnt;
}

static void update_cpu_topology(struct device *dev)
{
	int cluster_id[NR_CPUS] = {[0 ... NR_CPUS-1] = -1};
	cpumask_t cluster_cpus[NR_CPUS];
	uint32_t i;
	int cluster_cnt; //sync_cluster_cnt = 0;
	struct cluster_info *temp_ptr = NULL;

	cluster_info_probed = true;
	cluster_cnt = get_kernel_cluster_info(cluster_id, cluster_cpus);
	if (cluster_cnt <= 0) {
		cluster_cnt = get_device_tree_cluster_info(dev, cluster_id,
						cluster_cpus);
		if (cluster_cnt <= 0) {
			core_ptr = NULL;
			pr_debug("Cluster Info not defined. KTM continues.\n");
			return;
		}
	}

	core_ptr = devm_kzalloc(dev, sizeof(struct cluster_info), GFP_KERNEL);
	if (!core_ptr) {
		pr_err("Memory alloc failed\n");
		return;
	}
	core_ptr->parent_ptr = NULL;
	core_ptr->entity_count = cluster_cnt;
	core_ptr->cluster_id = -1;

	temp_ptr = devm_kzalloc(dev, sizeof(struct cluster_info) * cluster_cnt,
					GFP_KERNEL);
	if (!temp_ptr) {
		pr_err("Memory alloc failed\n");
		devm_kfree(dev, core_ptr);
		core_ptr = NULL;
		return;
	}

	for (i = 0; i < cluster_cnt; i++) {
		/* int idx = 0; */ //Unused

		pr_debug("Cluster_ID:%d CPU's:%lu\n", cluster_id[i],
				*cpumask_bits(&cluster_cpus[i]));
		temp_ptr[i].cluster_id = cluster_id[i];
		temp_ptr[i].parent_ptr = core_ptr;
		cpumask_copy(&temp_ptr[i].cluster_cores, &cluster_cpus[i]);
		temp_ptr[i].limited_max_freq = UINT_MAX;
		temp_ptr[i].limited_min_freq = 0;
		temp_ptr[i].freq_idx = 0;
		temp_ptr[i].freq_idx_low = 0;
		temp_ptr[i].freq_idx_high = 0;
		temp_ptr[i].freq_table = NULL;
		temp_ptr[i].entity_count = cpumask_weight(&cluster_cpus[i]);
		temp_ptr[i].child_entity_ptr = NULL;
	}
	core_ptr->child_entity_ptr = temp_ptr;
}

static int get_cpu_freq_plan_len(int cpu)
{
	int table_len = 0;
	struct device *cpu_dev = NULL;

	cpu_dev = get_cpu_device(cpu);
	if (!cpu_dev) {
		pr_err("Error in get CPU%d device\n", cpu);
		goto exit;
	}

	rcu_read_lock();
	table_len = dev_pm_opp_get_opp_count(cpu_dev);
	if (table_len <= 0) {
		pr_err("Error reading CPU%d freq table len. error:%d\n",
			cpu, table_len);
		table_len = 0;
		goto unlock_and_exit;
	}

unlock_and_exit:
	rcu_read_unlock();

exit:
	return table_len;
}

static int get_cpu_freq_plan(int cpu,
		 struct cpufreq_frequency_table *freq_table_ptr)
{
	int table_len = 0;
	struct dev_pm_opp *opp = NULL;
	unsigned long freq = 0;
	struct device *cpu_dev = NULL;

	cpu_dev = get_cpu_device(cpu);
	if (!cpu_dev) {
		pr_err("Error in get CPU%d device\n", cpu);
		goto exit;
	}

	rcu_read_lock();
	while (!IS_ERR(opp = dev_pm_opp_find_freq_ceil(cpu_dev, &freq))) {
		/* Convert from Hz to kHz */
		freq_table_ptr[table_len].frequency = freq / 1000;
		pr_debug("cpu%d freq %d :%d\n", cpu, table_len,
			freq_table_ptr[table_len].frequency);
		freq++;
		table_len++;
	}
	rcu_read_unlock();

exit:
	return table_len;
}

static int init_cluster_freq_table(void)
{
	uint32_t _cluster = 0;
	int table_len = 0;
	int ret = 0;
	struct cluster_info *cluster_ptr = NULL;

	for (; _cluster < core_ptr->entity_count; _cluster++, table_len = 0) {
		cluster_ptr = &core_ptr->child_entity_ptr[_cluster];
		if (cluster_ptr->freq_table)
			continue;

		table_len = get_cpu_freq_plan_len(
				cpumask_first(&cluster_ptr->cluster_cores));
		if (!table_len) {
			ret = -EAGAIN;
			continue;
		}
		cluster_ptr->freq_idx_low = 0;
		cluster_ptr->freq_idx_high = cluster_ptr->freq_idx =
				table_len - 1;
		if (cluster_ptr->freq_idx_high < 0
			|| (cluster_ptr->freq_idx_high
			< cluster_ptr->freq_idx_low)) {
			cluster_ptr->freq_idx = cluster_ptr->freq_idx_low =
				cluster_ptr->freq_idx_high = 0;
			WARN(1, "Cluster%d frequency table length:%d\n",
				cluster_ptr->cluster_id, table_len);
			ret = -EINVAL;
			goto exit;
		}
		cluster_ptr->freq_table = kzalloc(
			sizeof(struct cpufreq_frequency_table) * table_len,
			GFP_KERNEL);
		if (!cluster_ptr->freq_table) {
			pr_err("memory alloc failed\n");
			cluster_ptr->freq_idx = cluster_ptr->freq_idx_low =
				cluster_ptr->freq_idx_high = 0;
			ret = -ENOMEM;
			goto exit;
		}
		table_len = get_cpu_freq_plan(
				cpumask_first(&cluster_ptr->cluster_cores),
				cluster_ptr->freq_table);
		if (!table_len) {
			kfree(cluster_ptr->freq_table);
			cluster_ptr->freq_table = NULL;
			pr_err("Error reading cluster%d cpufreq table\n",
				cluster_ptr->cluster_id);
			ret = -EAGAIN;
			continue;
		}
	}

exit:
	return ret;
}

static void update_cluster_freq(void)
{
	int online_cpu = -1;
	struct cluster_info *cluster_ptr = NULL;
	uint32_t _cluster = 0, _cpu = 0, max = UINT_MAX, min = 0;
	uint32_t changed;

	if (!core_ptr)
		return;

	for (; _cluster < core_ptr->entity_count; _cluster++, _cpu = 0,
			online_cpu = -1, max = UINT_MAX, min = 0) {
		/*
		** Go over the frequency limits
		** of each core in the cluster and aggregate the minimum
		** and maximum frequencies. After aggregating, request for
		** frequency update on the first online core in that cluster.
		** Cpufreq driver takes care of updating the frequency of
		** other cores in a synchronous cluster.
		*/
		cluster_ptr = &core_ptr->child_entity_ptr[_cluster];

		for_each_cpu(_cpu, &cluster_ptr->cluster_cores) {
			if (online_cpu == -1 && cpu_online(_cpu))
				online_cpu = _cpu;
			max = min(max, cpus[_cpu].limited_max_freq);
			min = max(min, cpus[_cpu].limited_min_freq);
		}
		if (cluster_ptr->limited_max_freq == max
			&& cluster_ptr->limited_min_freq == min)
			continue;
		changed = 0;
		if (max != cluster_ptr->limited_max_freq)
			changed |= FREQ_LIMIT_MAX;
		if (min != cluster_ptr->limited_min_freq)
			changed |= FREQ_LIMIT_MIN;
		cluster_ptr->limited_max_freq = max;
		cluster_ptr->limited_min_freq = min;
		if (online_cpu == -1 && lmh_dcvs_available)
			online_cpu = cpumask_first(
					&cluster_ptr->cluster_cores);
		if (online_cpu != -1)
			update_cpu_freq(online_cpu, changed);
	}
}

static void do_cluster_freq_ctrl(int temp)
{
	uint32_t _cluster = 0;
	int _cpu = -1, freq_idx = 0;
	bool mitigate = false;
	struct cluster_info *cluster_ptr = NULL;

	if (temp >= msm_thermal_info.limit_temp_degC)
		mitigate = true;
	else if (temp < msm_thermal_info.limit_temp_degC -
		 msm_thermal_info.temp_hysteresis_degC)
		mitigate = false;
	else
		return;

	get_online_cpus();
	for (; _cluster < core_ptr->entity_count; _cluster++) {
		cluster_ptr = &core_ptr->child_entity_ptr[_cluster];
		if (!cluster_ptr->freq_table)
			continue;

		if (mitigate)
			freq_idx = max_t(int, cluster_ptr->freq_idx_low,
				(cluster_ptr->freq_idx
				- msm_thermal_info.bootup_freq_step));
		else
			freq_idx = min_t(int, cluster_ptr->freq_idx_high,
				(cluster_ptr->freq_idx
				+ msm_thermal_info.bootup_freq_step));
		if (freq_idx == cluster_ptr->freq_idx)
			continue;

		cluster_ptr->freq_idx = freq_idx;
		for_each_cpu(_cpu, &cluster_ptr->cluster_cores) {
			if (!(msm_thermal_info.bootup_freq_control_mask
				& BIT(_cpu)))
				continue;
			pr_info("Limiting CPU%d max frequency to %u. Temp:%d\n"
				, _cpu
				, cluster_ptr->freq_table[freq_idx].frequency
				, temp);
			cpus[_cpu].limited_max_freq = min(
				cluster_ptr->freq_table[freq_idx].frequency,
				cpus[_cpu].vdd_max_freq);
		}
	}
	if (_cpu != -1)
		update_cluster_freq();
	put_online_cpus();
}

/**
 * msm_thermal_lmh_dcvs_init: Initialize LMH DCVS hardware block
 *
 * @pdev: handle to the thermal device node
 *
 * Probe for the 'OSM clock' and initialize the LMH DCVS blocks.
 */
static int msm_thermal_lmh_dcvs_init(struct platform_device *pdev)
{
	struct clk *osm_clk;
	const char *clk_name = "osm";
	int ret = 0;

	/* We are okay if the osm clock is not present in DT */
	osm_clk = devm_clk_get(&pdev->dev, clk_name);
	if (IS_ERR(osm_clk))
		return ret;

	/*
	 * We actually don't need the clock, we just wanted to make sure
	 * the OSM block is ready.
	 */
	devm_clk_put(&pdev->dev, osm_clk);

	/* Enable the CRNT and Reliability algorithm. Again, we dont
	 * care if this fails
	 */
	ret = msm_lmh_dcvs_write(MSM_LIMITS_CLUSTER_0,
				MSM_LIMITS_SUB_FN_REL,
				MSM_LIMITS_ALGO_MODE_ENABLE, 1);
	if (ret)
		pr_err("Unable to enable REL algo for cluster0\n");
	ret = msm_lmh_dcvs_write(MSM_LIMITS_CLUSTER_1,
				MSM_LIMITS_SUB_FN_REL,
				MSM_LIMITS_ALGO_MODE_ENABLE, 1);
	if (ret)
		pr_err("Unable to enable REL algo for cluster1\n");

	ret = msm_lmh_dcvs_write(MSM_LIMITS_CLUSTER_0,
				MSM_LIMITS_SUB_FN_CRNT,
				MSM_LIMITS_ALGO_MODE_ENABLE, 1);
	if (ret)
		pr_err("Unable enable CRNT algo for cluster0\n");
	ret = msm_lmh_dcvs_write(MSM_LIMITS_CLUSTER_1,
				MSM_LIMITS_SUB_FN_CRNT,
				MSM_LIMITS_ALGO_MODE_ENABLE, 1);
	if (ret)
		pr_err("Unable enable CRNT algo for cluster1\n");

	lmh_dcvs_available = true;

	return ret;
}

/* If freq table exists, then we can send freq request */
static int check_freq_table(void)
{
	int ret = 0;
	static bool invalid_table;
	int table_len = 0;

	if (invalid_table)
		return -EINVAL;
	if (freq_table_get)
		return 0;

	if (core_ptr) {
		ret = init_cluster_freq_table();
		if (!ret)
			freq_table_get = 1;
		else if (ret == -EINVAL)
			invalid_table = true;
		goto exit;
	}

	table_len = get_cpu_freq_plan_len(0);
	if (!table_len)
		return -EINVAL;

	table = kzalloc(sizeof(struct cpufreq_frequency_table)
			* table_len, GFP_KERNEL);
	if (!table) {
		ret = -ENOMEM;
		goto exit;
	}
	table_len = get_cpu_freq_plan(0, table);
	if (!table_len) {
		pr_err("error reading cpufreq table\n");
		ret = -EINVAL;
		goto free_and_exit;
	}

	limit_idx_low = 0;
	limit_idx_high = limit_idx = table_len - 1;
	if (limit_idx_high < 0 || limit_idx_high < limit_idx_low) {
		invalid_table = true;
		limit_idx_low = limit_idx_high = limit_idx = 0;
		WARN(1, "CPU0 frequency table length:%d\n", table_len);
		ret = -EINVAL;
		goto free_and_exit;
	}
	freq_table_get = 1;

free_and_exit:
	if (ret) {
		kfree(table);
		table = NULL;
	}

exit:
	if (!ret) {
		int err;

		err = msm_thermal_lmh_dcvs_init(msm_thermal_info.pdev);
		if (err)
			pr_err("Error initializing OSM\n");
	}
	return ret;
}

static int update_cpu_min_freq_all(struct rail *apss_rail, uint32_t min)
{
	uint32_t cpu = 0, _cluster = 0, max_freq = UINT_MAX;
	int ret = 0;
	struct cluster_info *cluster_ptr = NULL;
	bool valid_table = false;

	if (!freq_table_get) {
		ret = check_freq_table();
		if (ret && !core_ptr) {
			pr_err("Fail to get freq table. err:%d\n", ret);
			return ret;
		}
	}
	if (min != apss_rail->min_level)
		max_freq = apss_rail->max_frequency_limit;

	get_online_cpus();
	/* If min is larger than allowed max */
	if (core_ptr) {
		for (; _cluster < core_ptr->entity_count; _cluster++) {
			cluster_ptr = &core_ptr->child_entity_ptr[_cluster];
			if (!cluster_ptr->freq_table)
				continue;
			valid_table = true;
			min = min(min,
				cluster_ptr->freq_table[
				cluster_ptr->freq_idx_high].frequency);
			max_freq = max(max_freq, cluster_ptr->freq_table[
				cluster_ptr->freq_idx_low].frequency);
		}
		if (!valid_table)
			goto update_freq_exit;
	} else {
		min = min(min, table[limit_idx_high].frequency);
		max_freq = max(max_freq, table[limit_idx_low].frequency);
	}

	pr_debug("Requesting min freq:%u max freq:%u for all CPU's\n",
		min, max_freq);
	if (freq_mitigation_task) {
		if (!apss_rail->device_handle[0]) {
			pr_err("device manager handle not registered\n");
			ret = -ENODEV;
			goto update_freq_exit;
		}
		for_each_possible_cpu(cpu) {
			cpus[cpu].vdd_max_freq = max_freq;
			apss_rail->request[cpu].freq.max_freq = max_freq;
			apss_rail->request[cpu].freq.min_freq = min;
			ret = devmgr_client_request_mitigation(
				apss_rail->device_handle[cpu],
				CPUFREQ_MITIGATION_REQ,
				&apss_rail->request[cpu]);
		}
	} else if (core_ptr) {
		for (_cluster = 0; _cluster < core_ptr->entity_count;
			_cluster++) {
			cluster_ptr = &core_ptr->child_entity_ptr[_cluster];
			if (!cluster_ptr->freq_table)
				continue;
			for_each_cpu(cpu, &cluster_ptr->cluster_cores) {
				uint32_t max;
				uint32_t changed = 0;

				cpus[cpu].vdd_max_freq = max_freq;
				max = min(cluster_ptr->freq_table[
					cluster_ptr->freq_idx].frequency,
					cpus[cpu].vdd_max_freq);

				if (max != cpus[cpu].limited_max_freq)
					changed |= FREQ_LIMIT_MAX;
				if (min != cpus[cpu].limited_min_freq)
					changed |= FREQ_LIMIT_MIN;

				cpus[cpu].limited_min_freq = min;
				cpus[cpu].limited_max_freq = max;
			}
			update_cluster_freq();
		}
	} else {
		for_each_possible_cpu(cpu) {
			uint32_t max;
			uint32_t changed = 0;

			cpus[cpu].vdd_max_freq = max_freq;
			max = min(table[limit_idx].frequency,
				cpus[cpu].vdd_max_freq);

			if (max != cpus[cpu].limited_max_freq)
				changed |= FREQ_LIMIT_MAX;
			if (min != cpus[cpu].limited_min_freq)
				changed |= FREQ_LIMIT_MIN;

			cpus[cpu].limited_min_freq = min;
			cpus[cpu].limited_max_freq = max;
		}
		update_cluster_freq();
	}

update_freq_exit:
	put_online_cpus();
	return ret;
}

static int vdd_restriction_apply_freq(struct rail *r, int level)
{
	int ret = 0;

	if (level == r->curr_level)
		return ret;

	/* level = -1: disable, level = 0,1,2..n: enable */
	if (level == -1) {
		ret = update_cpu_min_freq_all(r, r->min_level);
		if (ret)
			return ret;
		else
			r->curr_level = -1;
	} else if (level >= 0 && level < (r->num_levels)) {
		ret = update_cpu_min_freq_all(r, r->levels[level]);
		if (ret)
			return ret;
		else
			r->curr_level = level;
	} else {
		pr_err("level input:%d is not within range\n", level);
		return -EINVAL;
	}

	return ret;
}

static int vdd_restriction_apply_voltage(struct rail *r, int level)
{
	int ret = 0;

	if (r->reg == NULL) {
		pr_err("%s don't have regulator handle. can't apply vdd\n",
				r->name);
		return -EFAULT;
	}
	if (level == r->curr_level)
		return ret;

	/* level = -1: disable, level = 0,1,2..n: enable */
	if (level == -1) {
		ret = regulator_set_voltage(r->reg, r->min_level,
			INT_MAX);
		if (!ret)
			r->curr_level = -1;
		pr_debug("Requested min level for %s. curr level: %d\n",
				r->name, r->curr_level);
	} else if (level >= 0 && level < (r->num_levels)) {
		ret = regulator_set_voltage(r->reg, r->levels[level],
			INT_MAX);
		if (!ret)
			r->curr_level = level;
		pr_debug("Requesting level %d for %s. curr level: %d\n",
			r->levels[level], r->name, r->levels[r->curr_level]);
	} else {
		pr_err("level input:%d is not within range\n", level);
		return -EINVAL;
	}

	return ret;
}

/* Setting all rails the same mode */
static int psm_set_mode_all(int mode)
{
	int i = 0;
	int fail_cnt = 0;
	int ret = 0;

	pr_debug("Requesting PMIC Mode: %d\n", mode);
	for (i = 0; i < psm_rails_cnt; i++) {
		if (psm_rails[i].mode != mode) {
			ret = rpm_regulator_set_mode(psm_rails[i].reg, mode);
			if (ret) {
				pr_err("Cannot set mode:%d for %s. err:%d",
					mode, psm_rails[i].name, ret);
				fail_cnt++;
			} else
				psm_rails[i].mode = mode;
		}
	}

	return fail_cnt ? (-EFAULT) : ret;
}

static ssize_t vdd_rstr_en_show(
	struct kobject *kobj, struct kobj_attribute *attr, char *buf)
{
	struct vdd_rstr_enable *en = VDD_RSTR_ENABLE_FROM_ATTRIBS(attr);

	return snprintf(buf, PAGE_SIZE, "%d\n", en->enabled);
}

static ssize_t vdd_rstr_en_store(struct kobject *kobj,
	struct kobj_attribute *attr, const char *buf, size_t count)
{
	int ret = 0;
	int i = 0;
	uint8_t en_cnt = 0;
	uint8_t dis_cnt = 0;
	uint32_t val = 0;
	struct kernel_param kp;
	struct vdd_rstr_enable *en = VDD_RSTR_ENABLE_FROM_ATTRIBS(attr);

	mutex_lock(&vdd_rstr_mutex);
	kp.arg = &val;
	ret = param_set_bool(buf, &kp);
	if (ret) {
		pr_err("Invalid input %s for enabled\n", buf);
		goto done_vdd_rstr_en;
	}

	if ((val == 0) && (en->enabled == 0))
		goto done_vdd_rstr_en;

	for (i = 0; i < rails_cnt; i++) {
		if (rails[i].freq_req == 1 && freq_table_get)
			ret = vdd_restriction_apply_freq(&rails[i],
					(val) ? 0 : -1);
		else
			ret = vdd_restriction_apply_voltage(&rails[i],
			(val) ? 0 : -1);

		/*
		 * Even if fail to set one rail, still try to set the
		 * others. Continue the loop
		 */
		if (ret)
			pr_err("Set vdd restriction for %s failed\n",
					rails[i].name);
		else {
			if (val)
				en_cnt++;
			else
				dis_cnt++;
		}
	}
	/* As long as one rail is enabled, vdd rstr is enabled */
	if (val && en_cnt)
		en->enabled = 1;
	else if (!val && (dis_cnt == rails_cnt))
		en->enabled = 0;
	pr_debug("%s vdd restriction. curr: %d\n",
			(val) ? "Enable" : "Disable", en->enabled);

done_vdd_rstr_en:
	mutex_unlock(&vdd_rstr_mutex);
	return count;
}

static int send_temperature_band(enum msm_thermal_phase_ctrl phase,
	enum msm_temp_band req_band)
{
	int ret = 0;
	uint32_t msg_id;
	struct msm_rpm_request *rpm_req;
	unsigned int band = req_band;
	uint32_t key, resource, resource_id;

	if (phase < 0 || phase >= MSM_PHASE_CTRL_NR ||
		req_band <= 0 || req_band >= MSM_TEMP_MAX_NR) {
		pr_err("Invalid input\n");
		ret = -EINVAL;
		goto phase_ctrl_exit;
	}
	switch (phase) {
	case MSM_CX_PHASE_CTRL:
		key = msm_thermal_info.cx_phase_request_key;
		break;
	case MSM_GFX_PHASE_CTRL:
		key = msm_thermal_info.gfx_phase_request_key;
		break;
	default:
		goto phase_ctrl_exit;
		break;
	}

	resource = msm_thermal_info.phase_rpm_resource_type;
	resource_id = msm_thermal_info.phase_rpm_resource_id;
	pr_debug("Sending %s temperature band %d\n",
		(phase == MSM_CX_PHASE_CTRL) ? "CX" : "GFX",
		req_band);
	rpm_req = msm_rpm_create_request(MSM_RPM_CTX_ACTIVE_SET,
			resource, resource_id, 1);
	if (!rpm_req) {
		pr_err("Creating RPM request failed\n");
		ret = -ENXIO;
		goto phase_ctrl_exit;
	}

	ret = msm_rpm_add_kvp_data(rpm_req, key, (const uint8_t *)&band,
		(int)sizeof(band));
	if (ret) {
		pr_err("Adding KVP data failed. err:%d\n", ret);
		goto free_rpm_handle;
	}

	msg_id = msm_rpm_send_request(rpm_req);
	if (!msg_id) {
		pr_err("RPM send request failed\n");
		ret = -ENXIO;
		goto free_rpm_handle;
	}

	ret = msm_rpm_wait_for_ack(msg_id);
	if (ret) {
		pr_err("RPM wait for ACK failed. err:%d\n", ret);
		goto free_rpm_handle;
	}

free_rpm_handle:
	msm_rpm_free_request(rpm_req);
phase_ctrl_exit:
	return ret;
}

static uint32_t msm_thermal_str_to_int(const char *inp)
{
	int i, len;
	uint32_t output = 0;

	len = strnlen(inp, sizeof(uint32_t));
	for (i = 0; i < len; i++)
		output |= inp[i] << (i * 8);

	return output;
}

static ssize_t sensor_info_show(
	struct kobject *kobj, struct kobj_attribute *attr, char *buf)
{
	int i;
	ssize_t tot_size = 0, size = 0;

	for (i = 0; i < sensor_cnt; i++) {
		size = snprintf(&buf[tot_size], PAGE_SIZE - tot_size,
			"%s:%s:%s:%d ",
			sensors[i].type, sensors[i].name,
			sensors[i].alias ? : "",
			sensors[i].scaling_factor);
		if (tot_size + size >= PAGE_SIZE) {
			pr_err("Not enough buffer size\n");
			break;
		}
		tot_size += size;
	}
	if (tot_size)
		buf[tot_size - 1] = '\n';

	return tot_size;
}

static struct vdd_rstr_enable vdd_rstr_en = {
	.ko_attr.attr.name = __stringify(enabled),
	.ko_attr.attr.mode = 0644,
	.ko_attr.show = vdd_rstr_en_show,
	.ko_attr.store = vdd_rstr_en_store,
	.enabled = 1,
};

static struct attribute *vdd_rstr_en_attribs[] = {
	&vdd_rstr_en.ko_attr.attr,
	NULL,
};

static struct attribute_group vdd_rstr_en_attribs_gp = {
	.attrs  = vdd_rstr_en_attribs,
};

static ssize_t vdd_rstr_reg_value_show(
	struct kobject *kobj, struct kobj_attribute *attr, char *buf)
{
	int val = 0;
	struct rail *reg = VDD_RSTR_REG_VALUE_FROM_ATTRIBS(attr);
	/* -1:disabled, -2:fail to get regualtor handle */
	if (reg->curr_level < 0)
		val = reg->curr_level;
	else
		val = reg->levels[reg->curr_level];

	return snprintf(buf, PAGE_SIZE, "%d\n", val);
}

static ssize_t vdd_rstr_reg_level_show(
	struct kobject *kobj, struct kobj_attribute *attr, char *buf)
{
	struct rail *reg = VDD_RSTR_REG_LEVEL_FROM_ATTRIBS(attr);
	return snprintf(buf, PAGE_SIZE, "%d\n", reg->curr_level);
}

static ssize_t vdd_rstr_reg_level_store(struct kobject *kobj,
	struct kobj_attribute *attr, const char *buf, size_t count)
{
	int ret = 0;
	int val = 0;

	struct rail *reg = VDD_RSTR_REG_LEVEL_FROM_ATTRIBS(attr);

	mutex_lock(&vdd_rstr_mutex);
	if (vdd_rstr_en.enabled == 0)
		goto done_store_level;

	ret = kstrtouint(buf, 10, &val);
	if (ret) {
		pr_err("Invalid input %s for level\n", buf);
		goto done_store_level;
	}

	if (val < 0 || val > reg->num_levels - 1) {
		pr_err(" Invalid number %d for level\n", val);
		goto done_store_level;
	}

	if (val != reg->curr_level) {
		if (reg->freq_req == 1 && freq_table_get)
			update_cpu_min_freq_all(reg, reg->levels[val]);
		else {
